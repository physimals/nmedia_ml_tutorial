{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d1e4125",
   "metadata": {},
   "source": [
    "# This is a demo to illustrate the general pipeline of machine learning modelling, using a real-world binary classification problem (breast cancer prediction) as an example. \n",
    "\n",
    "# Dataset (https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic))\n",
    "\n",
    "## The Breast Cancer dataset (WDBC) is available in machine learning repository maintained by the University of California, Irvine. The dataset contains 569 samples of malignant and benign tumor cells. The first two columns in the dataset store the unique ID numbers of the samples and the corresponding diagnosis (M=malignant, B=benign), respectively. The columns 3-32 contain 30 real-value features that have been computed from digitized images of the cell nuclei, which can be used to build a model to predict whether a tumor is benign or malignant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf86057",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2293e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import sklearn\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279beb17",
   "metadata": {},
   "source": [
    "# Load WDBC data and visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdf98b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv file and display some rows\n",
    "all_df=pd.read_csv('WDBC.csv', index_col=False)\n",
    "all_df.head()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865a6058",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ID column is not useful, drop it\n",
    "all_df.drop('ID', axis=1, inplace=True)\n",
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7dceee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use info method to have a description of the data, e.g. instances, number of features, data types\n",
    "all_df.info()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4631ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic statistics of each column\n",
    "all_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb79d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distributions of benign and malignent as labeled in Column \"diagnosis\"\n",
    "\n",
    "all_df['Diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4813724d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a bar chart for each label\n",
    "\n",
    "sns.countplot(x=\"Diagnosis\", data=all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a497b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use box plot to check the value range and outliers of each feature\n",
    "\n",
    "data_mean = all_df.iloc[:, :]\n",
    "data_mean.plot(kind='box', subplots=True, layout=(8,4), sharex=False, sharey=False, fontsize=12, figsize=(15,20));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a228bec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the features data ranges\n",
    "# Only for the first 10 features, but try yourself to visualise more features\n",
    "\n",
    "fig,ax=plt.subplots(1,figsize=(20,8))\n",
    "sns.boxplot(data=all_df.iloc[:, 1:11],ax=ax)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593eab91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use boxplots to see if certain feature can discriminate between beign and malignant\n",
    "\n",
    "fig, axes = plt.subplots(nrows=8, ncols=4, figsize=(15,20))\n",
    "fig.subplots_adjust(hspace =.2, wspace=.5)\n",
    "axes = axes.ravel()\n",
    "for i, col in enumerate(all_df.columns[1:]):\n",
    "    _= sns.boxplot(y=col, x='Diagnosis', data=all_df, ax=axes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c7fdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix to observe the correlations between pair of features.\n",
    "\n",
    "corrMatt = all_df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corrMatt)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "fig, ax = plt.subplots(figsize=(20, 12))\n",
    "plt.title('Breast Cancer Feature Correlation')\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(260, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corrMatt, vmax=1.2, square=False, cmap=cmap, mask=mask, ax=ax, annot=True, fmt='.2g', linewidths=1);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f46655c",
   "metadata": {},
   "source": [
    "### What can you observe from the above correlation table?\n",
    "#### - The area of the tissue nucleus has a strong positive correlation with values of radius and perimeter.\n",
    "#### - Some paramters are moderately positive correlated (r between 0.5-0.75) are concavity and area, concavity and perimeter etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2830e349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots of the first 10 \"mean\" features. \n",
    "# You may try to plot the other features. \n",
    "\n",
    "sns.pairplot(all_df[list(all_df.columns[1:11]) + ['Diagnosis']], hue=\"Diagnosis\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d9640a",
   "metadata": {},
   "source": [
    "### What do you observe from the above scatter plots?\n",
    "#### - Mean values of cell radius, perimeter, area, compactness, concavity and concave points can be used in classification of the cancer. Larger values of these parameters tends to show a correlation with malignant tumors.\n",
    "#### - Histograms show that mean values of texture, smoothness, symmetry or fractual dimension does not show a particular preference of one diagnosis over the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96b89bd",
   "metadata": {},
   "source": [
    "# Data pre-processing and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c4724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign features to X \n",
    "X = all_df.drop('Diagnosis', axis=1)\n",
    "\n",
    "# Normalise the features to use zero mean normalisation\n",
    "# only for the first 10 features, but try yourself to visualise more features\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(X)\n",
    "fig,ax=plt.subplots(1,figsize=(20,8))\n",
    "sns.boxplot(data=Xs,ax=ax)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0347feab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA for dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "feature_names = list(X.columns)\n",
    "pca = PCA(n_components=10)\n",
    "Xs_pca = pca.fit_transform(Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0b2d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only retain the first two modes of PCA as the new features\n",
    "PCA_df = pd.DataFrame()\n",
    "PCA_df['PCA_1'] = Xs_pca[:,0]\n",
    "PCA_df['PCA_2'] = Xs_pca[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efa9026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the Malignant and Benign using the two PCA features. \n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(PCA_df['PCA_1'][all_df['Diagnosis'] == 'M'],PCA_df['PCA_2'][all_df['Diagnosis'] == 'M'],'ro', alpha = 0.7, markeredgecolor = 'k')\n",
    "plt.plot(PCA_df['PCA_1'][all_df['Diagnosis'] == 'B'],PCA_df['PCA_2'][all_df['Diagnosis'] == 'B'],'bo', alpha = 0.7, markeredgecolor = 'k')\n",
    "\n",
    "plt.xlabel('PCA_1')\n",
    "plt.ylabel('PCA_2')\n",
    "plt.legend(['Malignant','Benign']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef953662",
   "metadata": {},
   "source": [
    "# Predictive model using Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622c3da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, transform the class labels from their original string representation (M and B) into integers 1: M; 0: B\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "all_df['Diagnosis'] = le.fit_transform(all_df['Diagnosis'])\n",
    "all_df.head()\n",
    "\n",
    "# assign numerical label to y\n",
    "y = all_df['Diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b824a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then stratified sampling. Divide data into training and testing sets.\n",
    "# Pay attention that we are using the normalised data value Xs rather than X. You may try X.\n",
    "\n",
    "Xs_train, Xs_test, y_train, y_test = train_test_split(Xs, y, test_size=0.3, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0dfa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use kernal SVM classifier to train a model based on 70% of the data\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', probability=True)\n",
    "clf.fit(Xs_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c174cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify the test dataset and output the accuracy\n",
    "\n",
    "classifier_score = clf.score(Xs_test, y_test)\n",
    "print('The classifier accuracy score is {:03.2f}'.format(classifier_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5969e5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try K-fold cross validation\n",
    "# Get average of 5-fold cross-validation score using an SVM classifier.\n",
    "# Please try different number of folds and oberve the results\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "n_folds = 5\n",
    "clf_cv = SVC(C=1.0, kernel='rbf', degree=3, gamma='auto')\n",
    "cv_error = np.average(cross_val_score(clf_cv, Xs, y, cv=n_folds))\n",
    "print('The {}-fold cross-validation accuracy score for this classifier is {:.2f}'.format(n_folds, cv_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39238d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now Let's try classification with some selected features, not all the features\n",
    "# With 3 features, the classification accuracy is already quite good ~95%.\n",
    "# Try to include more features and observe.\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# model with just 3 best features selected (k=3)\n",
    "\n",
    "clf_fs_cv = Pipeline([\n",
    "    ('feature_selector', SelectKBest(f_classif, k=3)),\n",
    "    ('svc', SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', probability=True))\n",
    "])\n",
    "\n",
    "scores = cross_val_score(clf_fs_cv, Xs, y, cv=5)  # 5 folds.\n",
    "\n",
    "print(scores)\n",
    "avg = (100 * np.mean(scores), 100 * np.std(scores)/np.sqrt(scores.shape[0]))\n",
    "print(\"Average score and standard deviation: (%.2f +- %.3f)%%\"  %avg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c7e7a6",
   "metadata": {},
   "source": [
    "# Evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01dd2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use confusion matrix (TP, TN, FP, FN) to visualise the performance\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "y_pred = clf.fit(Xs_train, y_train).predict(Xs_test)\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea60ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix, \n",
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "ax.matshow(cm, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "     for j in range(cm.shape[1]):\n",
    "         ax.text(x=j, y=i,\n",
    "                s=cm[i, j], \n",
    "                va='center', ha='center')\n",
    "classes=[\"Benign\",\"Malignant\"]\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "plt.xlabel('Predicted Values', )\n",
    "plt.ylabel('Actual Values');\n",
    "print(classification_report(y_test, y_pred ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf89e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the receiver operating characteristic curve (ROC).\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "probas_ = clf.predict_proba(Xs_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probas_[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, lw=1, label='ROC fold (area = %0.2f)' % (roc_auc))\n",
    "plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Random')\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate | 1 - specificity (1 - Benign recall)')\n",
    "plt.ylabel('True Positive Rate | Sensitivity (Malignant recall)')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.axes().set_aspect(1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d58ec56",
   "metadata": {},
   "source": [
    "# Now let's try some other modelling methods: Logistic regression; K nearest neighbour, Gaussian Naive Bayes, Dicision Trees, Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b786d521",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "\n",
    "# Test options and evaluation metric\n",
    "num_folds = 5\n",
    "num_instances = len(Xs_train)\n",
    "scoring = 'accuracy'\n",
    "results = []\n",
    "names = []\n",
    "for name, model in tqdm(models):\n",
    "    kf = KFold(n_splits=num_folds)\n",
    "    cv_results = cross_val_score(model, Xs, y, cv=kf, scoring=scoring, n_jobs=-1)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "\n",
    "print('5-Fold cross-validation accuracy score for the training data for all the classifiers') \n",
    "for name, cv_results in zip(names, results):\n",
    "    print(\"%-10s: %.6f (%.6f)\" % (name, cv_results.mean(), cv_results.std()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da40e401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the algorithms\n",
    "plt.title( 'Algorithm Comparison' )\n",
    "plt.boxplot(results)\n",
    "plt.xlabel('Classifiers')\n",
    "plt.ylabel('5-Fold CV Scores')\n",
    "plt.xticks(np.arange(len(names)) + 1, names);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f522dd1",
   "metadata": {},
   "source": [
    "## It is observed that for this application, a simple linear regression works better than non-linear models\n",
    "## Here're some practice for you to try\n",
    "## 1- Observe the results if we don't normalise the feature value range\n",
    "## 2- Currently all the modelling methods (i.e. SVM, KNN, LG, etc) used default parameter settings. Check the related documents and try different settings to see if the performance can be improved.\n",
    "## 3- Check other feature selection methods and compare if the automatically selected features are consistent with the obervations using the correlation plots and box plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
